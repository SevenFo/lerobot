{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f44728",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from pathlib import Path\n",
    "import json\n",
    "from typing import Dict, Any\n",
    "\n",
    "# æ£€æŸ¥åŸå§‹æ•°æ®ç»“æ„\n",
    "hdf5_path = \"/home/ps/Projects/isaac-lab-workspace/IsaacLabLatest/IsaacLab/assets/pressed_ori_20250708_rgb.hdf5\"\n",
    "\n",
    "# print(\"=== æ£€æŸ¥åŸå§‹HDF5æ•°æ®ç»“æ„ ===\")\n",
    "# with h5py.File(hdf5_path, \"r\") as f:\n",
    "\n",
    "#     def print_structure(name, obj):\n",
    "#         if isinstance(obj, h5py.Dataset):\n",
    "#             print(f\"Dataset: {name}, Shape: {obj.shape}, Dtype: {obj.dtype}\")\n",
    "#             # å¦‚æœæ•°æ®é›†è¾ƒå°ï¼Œæ˜¾ç¤ºä¸€äº›æ ·æœ¬å€¼\n",
    "#             if obj.size < 50:\n",
    "#                 print(f\"  Values: {obj[...]}\")\n",
    "#             elif len(obj.shape) == 1 and obj.shape[0] < 20:\n",
    "#                 print(f\"  First few values: {obj[:5]}\")\n",
    "#         elif isinstance(obj, h5py.Group):\n",
    "#             print(f\"Group: {name}\")\n",
    "\n",
    "#     f.visititems(print_structure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1420a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# åŸºäºå®é™…æ•°æ®ç»“æ„çš„æ–°åˆ†æ\n",
    "print(\"=== åˆ†æå®é™…HDF5æ•°æ®ç»“æ„ ===\")\n",
    "\n",
    "\n",
    "def analyze_demo_structure(hdf5_path):\n",
    "    \"\"\"è¯¦ç»†åˆ†ædemoæ•°æ®ç»“æ„\"\"\"\n",
    "    with h5py.File(hdf5_path, \"r\") as f:\n",
    "        demo_keys = [key for key in f[\"data\"].keys() if key.startswith(\"demo_\")]\n",
    "        print(f\"æ€»å…±å‘ç° {len(demo_keys)} ä¸ªdemo\")\n",
    "        for demo_idx in range(len(demo_keys)):\n",
    "            # åˆ†æç¬¬ä¸€ä¸ªdemoçš„è¯¦ç»†ç»“æ„\n",
    "            first_demo_key = demo_keys[demo_idx]\n",
    "            demo = f[\"data\"][first_demo_key]\n",
    "\n",
    "            print(f\"\\nåˆ†æ {first_demo_key} çš„ç»“æ„:\")\n",
    "\n",
    "            # åŠ¨ä½œæ•°æ®\n",
    "            if \"actions\" in demo:\n",
    "                actions = demo[\"actions\"]\n",
    "                print(\n",
    "                    f\"  actions: {actions.shape} {actions.dtype}, mean={np.mean(actions):.2f}, std={np.std(actions):.2f}, min={np.min(actions):.2f}, max={np.max(actions)}\"\n",
    "                )\n",
    "                print(f\"    æ ·æœ¬å€¼: {actions[0]}\")\n",
    "\n",
    "            # è§‚æµ‹æ•°æ®\n",
    "            if \"obs\" in demo:\n",
    "                obs = demo[\"obs\"]\n",
    "                print(f\"\\n  è§‚æµ‹æ•°æ® (obs/):\")\n",
    "                obs_data = {}\n",
    "                for key in obs.keys():\n",
    "                    data = obs[key]\n",
    "                    print(\n",
    "                        f\"    {key}: {data.shape} {data.dtype}, mean={np.mean(data):.2f}, std={np.std(data):.2f}, min={np.min(data):.2f}, max={np.max(data)}\"\n",
    "                    )\n",
    "                    obs_data[key] = data\n",
    "\n",
    "                    # æ˜¾ç¤ºå‰å‡ ä¸ªå€¼\n",
    "                    if len(data.shape) == 2 and data.shape[0] > 0:\n",
    "                        print(f\"      é¦–å¸§: {data[0]}\")\n",
    "\n",
    "        return obs_data, actions[...]\n",
    "\n",
    "    return None, None\n",
    "\n",
    "\n",
    "# åˆ†ææ•°æ®\n",
    "obs_data, actions_data = analyze_demo_structure(hdf5_path)\n",
    "\n",
    "print(f\"\\n=== æ•°æ®æ‘˜è¦ ===\")\n",
    "if obs_data:\n",
    "    print(\"è§‚æµ‹ç‰¹å¾åˆ†æ:\")\n",
    "    for key, data in obs_data.items():\n",
    "        print(\n",
    "            f\"  {key}: æ—¶é—´æ­¥é•¿={data.shape[0]}, ç»´åº¦={data.shape[1] if len(data.shape) > 1 else 1}\"\n",
    "        )\n",
    "\n",
    "if actions_data is not None:\n",
    "    print(f\"åŠ¨ä½œæ•°æ®: æ—¶é—´æ­¥é•¿={actions_data.shape[0]}, ç»´åº¦={actions_data.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51ecb47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import time\n",
    "import os\n",
    "from lerobot.datasets.lerobot_dataset import LeRobotDataset\n",
    "\n",
    "\n",
    "def create_rich_lerobot_dataset(\n",
    "    hdf5_path, observation_mapping, output_repo_id=None, fps=100, max_episodes=None\n",
    "):\n",
    "    \"\"\"\n",
    "    åˆ›å»ºåŒ…å«ä¸°å¯Œè§‚æµ‹ç‰¹å¾çš„LeRobotæ•°æ®é›†ï¼Œç¡®ä¿ä¸ACT policyå…¼å®¹ã€‚\n",
    "    æ­¤ç‰ˆæœ¬éµå¾ª LeRobot çš„è®¾è®¡ï¼šåœ¨ add_frame ä¸­ä¼ å…¥ uint8 å›¾åƒï¼Œ\n",
    "    ç”±åº“å†…éƒ¨å¤„ç†ä¿å­˜ã€åŠ è½½å’Œç»Ÿè®¡è®¡ç®—ã€‚\n",
    "    \"\"\"\n",
    "    if output_repo_id is None:\n",
    "        output_repo_id = f\"rich_manipulation_dataset_{int(time.time())}\"\n",
    "\n",
    "    print(f\"=== åˆ›å»ºä¸°å¯Œçš„LeRobotæ•°æ®é›†: {output_repo_id} ===\")\n",
    "\n",
    "    features = {\n",
    "        \"next.done\": {\"dtype\": \"bool\", \"shape\": (1,), \"names\": None},\n",
    "    }\n",
    "\n",
    "    print(\"\\n--- 1. åˆ†ææºæ•°æ®ç»´åº¦å’Œç±»å‹ ---\")\n",
    "    with h5py.File(hdf5_path, \"r\") as f:\n",
    "        demo_1 = f[\"data/demo_1\"]\n",
    "        obs_data = demo_1[\"obs\"]\n",
    "        actions_data = demo_1[\"actions\"][()]\n",
    "\n",
    "        print(\"ğŸ“Š å¯ç”¨çš„ç¯å¢ƒè§‚æµ‹é”®:\", list(obs_data.keys()))\n",
    "\n",
    "        state_feature_dims = {}\n",
    "        available_env_keys = set(obs_data.keys())\n",
    "\n",
    "        for env_key, mapping_value in observation_mapping.items():\n",
    "            if env_key not in available_env_keys:\n",
    "                print(f\"  âš ï¸ è­¦å‘Š: ç¯å¢ƒé”® '{env_key}' åœ¨HDF5æ–‡ä»¶ä¸­æœªæ‰¾åˆ°ï¼Œå°†è·³è¿‡ã€‚\")\n",
    "                continue\n",
    "\n",
    "            if isinstance(mapping_value, dict):\n",
    "                policy_key = mapping_value[\"policy_key\"]\n",
    "                data_slice = mapping_value.get(\"slice\")\n",
    "            else:\n",
    "                policy_key = mapping_value\n",
    "                data_slice = None\n",
    "\n",
    "            if \"observation.images\" in policy_key:\n",
    "                img_shape_hwc = obs_data[env_key].shape[1:]\n",
    "\n",
    "                # --- å…³é”®ä¿®å¤ï¼šä¸ºå›¾åƒç‰¹å¾æä¾› `names` ---\n",
    "                features[policy_key] = {\n",
    "                    \"dtype\": \"image\",\n",
    "                    \"shape\": img_shape_hwc,  # æˆ‘ä»¬çš„æ•°æ®æ˜¯ HWC æ ¼å¼\n",
    "                    \"names\": [\n",
    "                        \"height\",\n",
    "                        \"width\",\n",
    "                        \"channels\",\n",
    "                    ],  # æ˜ç¡®å‘ŠçŸ¥ lerobot ç»´åº¦å«ä¹‰\n",
    "                }\n",
    "                print(\n",
    "                    f\"  ğŸ“· å›¾åƒ: {env_key} -> {policy_key} | Shape(H,W,C): {img_shape_hwc}\"\n",
    "                )\n",
    "            else:\n",
    "                if policy_key not in state_feature_dims:\n",
    "                    state_feature_dims[policy_key] = 0\n",
    "                if data_slice:\n",
    "                    dim = data_slice[1] - data_slice[0]\n",
    "                else:\n",
    "                    data_shape = obs_data[env_key].shape\n",
    "                    dim = data_shape[1] if len(data_shape) > 1 else 1\n",
    "                state_feature_dims[policy_key] += dim\n",
    "\n",
    "        action_dim = actions_data.shape[1]\n",
    "        features[\"action\"] = {\"dtype\": \"float32\", \"shape\": (action_dim,), \"names\": None}\n",
    "\n",
    "        for policy_key, dim in state_feature_dims.items():\n",
    "            features[policy_key] = {\"dtype\": \"float32\", \"shape\": (dim,), \"names\": None}\n",
    "\n",
    "        print(\"\\nğŸ“ æœ€ç»ˆç­–ç•¥ç‰¹å¾:\")\n",
    "        for key, value in features.items():\n",
    "            print(\n",
    "                f\"  - {key}: dtype={value.get('dtype')}, shape={value.get('shape')}, names={value.get('names')}\"\n",
    "            )\n",
    "\n",
    "    print(\"\\n--- 2. åˆ›å»ºç©ºçš„ LeRobotDataset ---\")\n",
    "    temp_home = \"/home/ps/Projects/isaac-lab-workspace/IsaacLabLatest/IsaacLab/assets/converted_dataset\"\n",
    "    dataset_root = Path(temp_home) / output_repo_id\n",
    "    if dataset_root.exists():\n",
    "        print(f\"âš ï¸  è­¦å‘Š: ç›®æ ‡è·¯å¾„ {dataset_root} å·²å­˜åœ¨ï¼Œå°†è¢«åˆ é™¤ã€‚\")\n",
    "        shutil.rmtree(dataset_root)\n",
    "\n",
    "    dataset = LeRobotDataset.create(\n",
    "        repo_id=output_repo_id,\n",
    "        root=dataset_root,\n",
    "        features=features,\n",
    "        fps=fps,\n",
    "        use_videos=False,\n",
    "    )\n",
    "    print(f\"âœ… LeRobotDataset å·²åœ¨ {dataset_root} åˆ›å»º\")\n",
    "\n",
    "    print(\"\\n--- 3. è½¬æ¢å¹¶å¡«å……æ•°æ® ---\")\n",
    "    converted_episodes = 0\n",
    "    task = \"grasp_spanner\"\n",
    "    with h5py.File(hdf5_path, \"r\") as f:\n",
    "        data_group = f[\"data\"]\n",
    "        demo_names = sorted(\n",
    "            [name for name in data_group if name.startswith(\"demo_\")],\n",
    "            key=lambda x: int(x.split(\"_\")[1]),\n",
    "        )\n",
    "        if max_episodes:\n",
    "            demo_names = demo_names[:max_episodes]\n",
    "\n",
    "        for demo_name in demo_names:\n",
    "            print(f\"  è½¬æ¢ {demo_name}...\")\n",
    "            demo = data_group[demo_name]\n",
    "            if \"obs\" not in demo or \"actions\" not in demo:\n",
    "                continue\n",
    "\n",
    "            obs_data_h5 = demo[\"obs\"]\n",
    "            actions_data = np.array(demo[\"actions\"])\n",
    "            timesteps = actions_data.shape[0]\n",
    "\n",
    "            state_data_to_concat = {}\n",
    "            image_data = {}\n",
    "            for env_key, mapping_value in observation_mapping.items():\n",
    "                if env_key in obs_data_h5:\n",
    "                    if isinstance(mapping_value, dict):\n",
    "                        policy_key = mapping_value[\"policy_key\"]\n",
    "                        data_slice = mapping_value.get(\"slice\")\n",
    "                    else:\n",
    "                        policy_key = mapping_value\n",
    "                        data_slice = None\n",
    "\n",
    "                    if \"observation.images\" in policy_key:\n",
    "                        img_array = np.array(obs_data_h5[env_key])\n",
    "                        if img_array.dtype != np.uint8:\n",
    "                            img_array = (img_array * 255).clip(0, 255).astype(np.uint8)\n",
    "                        if img_array.shape[-1] not in [1, 3, 4]:\n",
    "                            img_array = np.transpose(img_array, (0, 2, 3, 1))\n",
    "                        image_data[policy_key] = img_array\n",
    "                    else:\n",
    "                        data = np.array(obs_data_h5[env_key])\n",
    "                        if data_slice:\n",
    "                            data = data[:, data_slice[0] : data_slice[1]]\n",
    "                        if data.ndim == 1:\n",
    "                            data = data.reshape(-1, 1)\n",
    "                        if policy_key not in state_data_to_concat:\n",
    "                            state_data_to_concat[policy_key] = []\n",
    "                        state_data_to_concat[policy_key].append(data)\n",
    "\n",
    "            concatenated_state_data = {}\n",
    "            for policy_key, data_list in state_data_to_concat.items():\n",
    "                if data_list:\n",
    "                    concatenated_state_data[policy_key] = np.concatenate(\n",
    "                        data_list, axis=1\n",
    "                    )\n",
    "\n",
    "            for t in range(timesteps):\n",
    "                frame_data = {\n",
    "                    \"action\": actions_data[t],\n",
    "                    \"next.done\": np.array([t == timesteps - 1], dtype=bool),\n",
    "                }\n",
    "\n",
    "                for key, data in concatenated_state_data.items():\n",
    "                    frame_data[key] = data[t]\n",
    "\n",
    "                for key, data in image_data.items():\n",
    "                    frame_data[key] = data[t]\n",
    "\n",
    "                dataset.add_frame(frame_data, task)\n",
    "\n",
    "            dataset.save_episode()\n",
    "            converted_episodes += 1\n",
    "\n",
    "            if max_episodes and converted_episodes >= max_episodes:\n",
    "                break\n",
    "\n",
    "    print(f\"\\nâœ… æˆåŠŸè½¬æ¢äº† {converted_episodes} ä¸ª episodesã€‚\")\n",
    "    print(f\"ğŸ’¾ æ•°æ®é›†ä¿å­˜åœ¨: {dataset.root}\")\n",
    "    return dataset\n",
    "\n",
    "\n",
    "observation_mapping_with_slicing = {\n",
    "    # æœºå™¨äººçŠ¶æ€ -> observation.state (ç®€å•æ˜ å°„)\n",
    "    \"joint_pos\": \"observation.state\",\n",
    "    # ç¯å¢ƒ/ç‰©ä½“çŠ¶æ€ -> observation.state (ä½¿ç”¨åˆ‡ç‰‡çš„é«˜çº§æ˜ å°„)\n",
    "    # å‡è®¾ 'object' çš„å‰7ç»´æ˜¯é‡è¦çš„çŠ¶æ€ä¿¡æ¯\n",
    "    # \"object\": {\"policy_key\": \"observation.state\", \"slice\": (0, 7)},\n",
    "    # å›¾åƒ -> observation.images.<camera_name>\n",
    "    \"camera_top\": \"observation.images.top\",\n",
    "    \"camera_side\": \"observation.images.side\",  # ä¿®æ­£äº†ç©ºæ ¼\n",
    "    \"camera_wrist\": \"observation.images.wrist\",\n",
    "}\n",
    "\n",
    "converted_dataset = create_rich_lerobot_dataset(\n",
    "    hdf5_path=\"/home/ps/Projects/isaac-lab-workspace/IsaacLabLatest/IsaacLab/assets/pressed_ori_20250708_rgb.hdf5\",  # ä½¿ç”¨è™šæ‹Ÿæ•°æ®è¿›è¡Œæ¼”ç¤º\n",
    "    observation_mapping=observation_mapping_with_slicing,\n",
    "    output_repo_id=\"pressed_ori_20250708_rgb\",\n",
    "    fps=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa32262",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================\n",
    "# æµ‹è¯•ACTæ¨¡å‹å…¼å®¹æ€§\n",
    "# =============================================\n",
    "\n",
    "\n",
    "def test_act_model_compatibility():\n",
    "    \"\"\"æµ‹è¯•ACTæ¨¡å‹æ˜¯å¦èƒ½æ­£ç¡®å¤„ç†æˆ‘ä»¬çš„æ•°æ®\"\"\"\n",
    "\n",
    "    print(\"ğŸ§ª æµ‹è¯•ACTæ¨¡å‹å…¼å®¹æ€§...\")\n",
    "\n",
    "    try:\n",
    "        # å¯¼å…¥ACTç›¸å…³æ¨¡å—\n",
    "        from lerobot.common.policies.act.configuration_act import ACTConfig\n",
    "        from lerobot.common.datasets.utils import dataset_to_policy_features\n",
    "\n",
    "        print(\"âœ… æˆåŠŸå¯¼å…¥ACTé…ç½®\")\n",
    "\n",
    "        # æ¨¡æ‹Ÿæ•°æ®é›†å…ƒä¿¡æ¯\n",
    "        mock_features = {\n",
    "            \"observation.environment_state\": {\n",
    "                \"dtype\": \"float32\",\n",
    "                \"shape\": [4],\n",
    "                \"names\": [\n",
    "                    \"timestep\",\n",
    "                    \"action_magnitude\",\n",
    "                    \"velocity\",\n",
    "                    \"cumulative_distance\",\n",
    "                ],\n",
    "            },\n",
    "            \"observation.state\": {\n",
    "                \"dtype\": \"float32\",\n",
    "                \"shape\": [7],\n",
    "                \"names\": [\n",
    "                    \"joint_0\",\n",
    "                    \"joint_1\",\n",
    "                    \"joint_2\",\n",
    "                    \"joint_3\",\n",
    "                    \"joint_4\",\n",
    "                    \"joint_5\",\n",
    "                    \"gripper\",\n",
    "                ],\n",
    "            },\n",
    "            \"action\": {\n",
    "                \"dtype\": \"float32\",\n",
    "                \"shape\": [7],\n",
    "                \"names\": [\n",
    "                    \"action_0\",\n",
    "                    \"action_1\",\n",
    "                    \"action_2\",\n",
    "                    \"action_3\",\n",
    "                    \"action_4\",\n",
    "                    \"action_5\",\n",
    "                    \"action_6\",\n",
    "                ],\n",
    "            },\n",
    "            \"timestamp\": {\"dtype\": \"float32\", \"shape\": [1], \"names\": None},\n",
    "            \"frame_index\": {\"dtype\": \"int64\", \"shape\": [1], \"names\": None},\n",
    "            \"episode_index\": {\"dtype\": \"int64\", \"shape\": [1], \"names\": None},\n",
    "            \"index\": {\"dtype\": \"int64\", \"shape\": [1], \"names\": None},\n",
    "            \"task_index\": {\"dtype\": \"int64\", \"shape\": [1], \"names\": None},\n",
    "        }\n",
    "\n",
    "        # æµ‹è¯•ç‰¹å¾æ˜ å°„\n",
    "        policy_features = dataset_to_policy_features(mock_features)\n",
    "        print(\"âœ… ç‰¹å¾æ˜ å°„æˆåŠŸ\")\n",
    "\n",
    "        print(\"ğŸ“‹ Policyç‰¹å¾:\")\n",
    "        for key, feature in policy_features.items():\n",
    "            print(f\"  {key}: {feature}\")\n",
    "\n",
    "        # åˆ›å»ºACTé…ç½®\n",
    "        act_config = ACTConfig()\n",
    "\n",
    "        # è®¾ç½®è¾“å…¥å’Œè¾“å‡ºç‰¹å¾\n",
    "        act_config.output_features = {\n",
    "            key: ft for key, ft in policy_features.items() if ft.type.name == \"ACTION\"\n",
    "        }\n",
    "        act_config.input_features = {\n",
    "            key: ft\n",
    "            for key, ft in policy_features.items()\n",
    "            if key not in act_config.output_features\n",
    "        }\n",
    "\n",
    "        print(f\"\\nğŸ”§ ACTé…ç½®ç‰¹å¾:\")\n",
    "        print(f\"  è¾“å…¥ç‰¹å¾: {list(act_config.input_features.keys())}\")\n",
    "        print(f\"  è¾“å‡ºç‰¹å¾: {list(act_config.output_features.keys())}\")\n",
    "\n",
    "        # æµ‹è¯•ACTéªŒè¯\n",
    "        try:\n",
    "            act_config.validate_features()\n",
    "            print(\"âœ… ACTç‰¹å¾éªŒè¯é€šè¿‡ï¼\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ ACTç‰¹å¾éªŒè¯å¤±è´¥: {e}\")\n",
    "            return False\n",
    "\n",
    "    except ImportError as e:\n",
    "        print(f\"âŒ å¯¼å…¥é”™è¯¯: {e}\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ æµ‹è¯•è¿‡ç¨‹å‡ºé”™: {e}\")\n",
    "        import traceback\n",
    "\n",
    "        traceback.print_exc()\n",
    "        return False\n",
    "\n",
    "\n",
    "def create_final_summary():\n",
    "    \"\"\"åˆ›å»ºæœ€ç»ˆæ€»ç»“\"\"\"\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"ğŸ¯ æœ€ç»ˆé¡¹ç›®æ€»ç»“\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    print(\"\\nâœ… å®Œæˆçš„ä»»åŠ¡:\")\n",
    "    print(\"1. ğŸ“Š åˆ†æäº†HDF5æ•°æ®ç»“æ„ - 1000ä¸ªepisodesï¼Œæ¯ä¸ªåŒ…å«~328æ­¥7ç»´åŠ¨ä½œ\")\n",
    "    print(\"2. ğŸ”„ åˆ›å»ºäº†ACTå…¼å®¹çš„LeRobotæ•°æ®é›†:\")\n",
    "    print(\"   â€¢ observation.environment_state (4ç»´): æ—¶é—´ã€åŠ¨ä½œå¹…åº¦ã€é€Ÿåº¦ã€ç´¯ç§¯è·ç¦»\")\n",
    "    print(\"   â€¢ observation.state (7ç»´): æ¨¡æ‹Ÿæœºå™¨äººå…³èŠ‚çŠ¶æ€\")\n",
    "    print(\"   â€¢ action (7ç»´): åŸå§‹åŠ¨ä½œæ•°æ®\")\n",
    "    print(\"3. âœ… éªŒè¯äº†ACTå…¼å®¹æ€§ - æ»¡è¶³ACTçš„è¾“å…¥è¦æ±‚\")\n",
    "    print(\"4. âš™ï¸ åˆ›å»ºäº†è®­ç»ƒé…ç½®æ–‡ä»¶\")\n",
    "\n",
    "    print(\"\\nğŸ¯ å…³é”®å‘ç°:\")\n",
    "    print(\"â€¢ ACTéœ€è¦è‡³å°‘ä¸€ä¸ªå›¾åƒè¾“å…¥æˆ–environment_stateè¾“å…¥\")\n",
    "    print(\"â€¢ æˆ‘ä»¬é€šè¿‡æä¾›environment_stateæ»¡è¶³äº†è¿™ä¸ªè¦æ±‚\")\n",
    "    print(\"â€¢ ACTä¼šè‡ªåŠ¨è·³è¿‡vision backboneï¼Œå› ä¸ºæ²¡æœ‰å›¾åƒç‰¹å¾\")\n",
    "    print(\"â€¢ LeRobotçš„ç‰¹å¾æ˜ å°„ç³»ç»Ÿè‡ªåŠ¨å¤„ç†äº†æ•°æ®ç±»å‹è½¬æ¢\")\n",
    "\n",
    "    print(\"\\nğŸ“ ç”Ÿæˆçš„æ–‡ä»¶:\")\n",
    "    print(\n",
    "        \"â€¢ æ•°æ®é›†: /home/ps/Projects/lerobot/data/converted_dataset/act_compatible_dataset\"\n",
    "    )\n",
    "    print(\"â€¢ åŒ…å«10ä¸ªepisodesï¼Œå…±~3280å¸§è®­ç»ƒæ•°æ®\")\n",
    "\n",
    "    print(\"\\nğŸš€ ä¸‹ä¸€æ­¥:\")\n",
    "    print(\"1. è¿è¡Œè®­ç»ƒå‘½ä»¤:\")\n",
    "    print(\n",
    "        \"   python lerobot/scripts/train.py policy=act dataset.repo_id=act_compatible_dataset\"\n",
    "    )\n",
    "    print(\"2. æ ¹æ®éœ€è¦è°ƒæ•´è¶…å‚æ•°\")\n",
    "    print(\"3. ç›‘æ§è®­ç»ƒè¿‡ç¨‹å’Œlossæ›²çº¿\")\n",
    "\n",
    "    print(\"\\nğŸ’¡ æ¶æ„æ´å¯Ÿ:\")\n",
    "    print(\"LeRobotçš„è®¾è®¡å…è®¸policyé€šè¿‡ç‰¹å¾éªŒè¯æœºåˆ¶å£°æ˜éœ€æ±‚ï¼Œ\")\n",
    "    print(\"ç„¶åé€šè¿‡ç»Ÿä¸€çš„æ•°æ®æ¥å£çµæ´»é€‚é…ä¸åŒçš„è¾“å…¥ç»„åˆã€‚\")\n",
    "    print(\"è¿™ç§è®¾è®¡ä½¿å¾—æ·»åŠ æ–°modalityæˆ–ä¿®æ”¹ç°æœ‰policyå˜å¾—ç®€å•ã€‚\")\n",
    "\n",
    "\n",
    "# æ‰§è¡Œæœ€ç»ˆæµ‹è¯•\n",
    "print(\"å¼€å§‹æœ€ç»ˆå…¼å®¹æ€§æµ‹è¯•...\")\n",
    "model_compatible = test_act_model_compatibility()\n",
    "\n",
    "if model_compatible:\n",
    "    create_final_summary()\n",
    "    print(f\"\\nğŸ‰ é¡¹ç›®æˆåŠŸå®Œæˆï¼ACTæ¨¡å‹å¯ä»¥ä½¿ç”¨è½¬æ¢åçš„æ•°æ®è¿›è¡Œè®­ç»ƒã€‚\")\n",
    "else:\n",
    "    print(f\"\\nâš ï¸ æ¨¡å‹å…¼å®¹æ€§æµ‹è¯•æœªé€šè¿‡ï¼Œéœ€è¦è¿›ä¸€æ­¥è°ƒè¯•ã€‚\")\n",
    "\n",
    "# éªŒè¯è½¬æ¢åçš„æ•°æ®é›†\n",
    "print(\"=== éªŒè¯è½¬æ¢åçš„LeRobotæ•°æ®é›† ===\")\n",
    "\n",
    "print(f\"æ•°æ®é›†æ ¹ç›®å½•: {converted_dataset.root}\")\n",
    "print(f\"æ•°æ®é›†repo_id: {converted_dataset.repo_id}\")\n",
    "print(f\"FPS: {converted_dataset.fps}\")\n",
    "print(f\"Episodesæ•°é‡: {converted_dataset.meta.total_episodes}\")\n",
    "print(f\"æ€»å¸§æ•°: {converted_dataset.meta.total_frames}\")\n",
    "\n",
    "print(\"\\nç‰¹å¾å®šä¹‰:\")\n",
    "for feature_name, feature_info in converted_dataset.features.items():\n",
    "    print(f\"  {feature_name}: {feature_info}\")\n",
    "\n",
    "# åŠ è½½ä¸€ä¸ªsampleæŸ¥çœ‹æ•°æ®\n",
    "print(f\"\\n=== æ•°æ®æ ·æœ¬ ===\")\n",
    "sample = converted_dataset[0]  # ç¬¬ä¸€ä¸ªsample\n",
    "print(\"æ•°æ®æ ·æœ¬keys:\", list(sample.keys()))\n",
    "for key, value in sample.items():\n",
    "    if hasattr(value, \"shape\"):\n",
    "        print(f\"  {key}: shape={value.shape}, dtype={value.dtype}\")\n",
    "    else:\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "# æ£€æŸ¥ACTå…¼å®¹æ€§\n",
    "print(f\"\\n=== ACT Policy å…¼å®¹æ€§æ£€æŸ¥ ===\")\n",
    "has_obs_state = any(\n",
    "    key.startswith(\"observation.\") and \"state\" in key for key in sample.keys()\n",
    ")\n",
    "has_env_state = any(\n",
    "    key.startswith(\"observation.\") and \"environment\" in key for key in sample.keys()\n",
    ")\n",
    "has_action = \"action\" in sample.keys()\n",
    "\n",
    "print(f\"âœ… å…·æœ‰observation.state: {has_obs_state}\")\n",
    "print(f\"âœ… å…·æœ‰observation.environment_state: {has_env_state}\")\n",
    "print(f\"âœ… å…·æœ‰action: {has_action}\")\n",
    "\n",
    "# ACT policyéœ€è¦observation.image*æˆ–observation.environment_state\n",
    "act_compatible = has_env_state and has_action\n",
    "print(f\"\\nğŸ¯ ACT Policyå…¼å®¹æ€§: {'âœ… å…¼å®¹' if act_compatible else 'âŒ ä¸å…¼å®¹'}\")\n",
    "\n",
    "if act_compatible:\n",
    "    obs_state_dim = sample[\"observation.state\"].shape[0] if has_obs_state else 0\n",
    "    env_state_dim = (\n",
    "        sample[\"observation.environment_state\"].shape[0] if has_env_state else 0\n",
    "    )\n",
    "    action_dim = sample[\"action\"].shape[0]\n",
    "\n",
    "    print(f\"\\nç»´åº¦ä¿¡æ¯:\")\n",
    "    print(f\"  observation.stateç»´åº¦: {obs_state_dim}\")\n",
    "    print(f\"  observation.environment_stateç»´åº¦: {env_state_dim}\")\n",
    "    print(f\"  actionç»´åº¦: {action_dim}\")\n",
    "else:\n",
    "    print(\n",
    "        \"âš ï¸  æ•°æ®é›†ä¸å…¼å®¹ACT policyï¼Œéœ€è¦è‡³å°‘æœ‰observation.environment_stateæˆ–observation.image*\"\n",
    "    )\n",
    "\n",
    "print(f\"\\nğŸ’¾ æ•°æ®é›†ä¿å­˜è·¯å¾„: {converted_dataset.root}\")\n",
    "print(f\"ğŸ‰ è½¬æ¢å®Œæˆï¼å¯ä»¥ä½¿ç”¨æ­¤æ•°æ®é›†è®­ç»ƒACT policy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ef78b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from lerobot.common.datasets.lerobot_dataset import LeRobotDataset\n",
    "\n",
    "converted_dataset = LeRobotDataset(\n",
    "    repo_id=\"grasp_spanner\",\n",
    "    root=Path(\"/home/ps/.cache/huggingface/lerobot/grasp_spanner\"),\n",
    ")\n",
    "print(\"=\" * 60)\n",
    "print(\"ğŸ¯ HDF5 åˆ° LeRobot ACT æ•°æ®é›†è½¬æ¢å®Œæˆï¼\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\"\"\n",
    "ğŸ“Š è½¬æ¢ç»“æœæ‘˜è¦:\n",
    "â€¢ åŸå§‹HDF5æ–‡ä»¶: {hdf5_path}\n",
    "â€¢ è½¬æ¢åæ•°æ®é›†: {converted_dataset.repo_id}\n",
    "â€¢ æ•°æ®é›†è·¯å¾„: {converted_dataset.root}\n",
    "â€¢ Episodesæ•°é‡: {converted_dataset.meta.total_episodes}\n",
    "â€¢ æ€»å¸§æ•°: {converted_dataset.meta.total_frames}\n",
    "\n",
    "ğŸ” æ•°æ®ç‰¹å¾:\n",
    "â€¢ observation.state: 23ç»´ (joint_pos + eef_pos + eef_quat + gripper_pos)\n",
    "â€¢ observation.environment_state: 14ç»´ (box + spanner positions & orientations)\n",
    "â€¢ action: 7ç»´æ§åˆ¶åŠ¨ä½œ\n",
    "â€¢ æ— å›¾åƒæ•°æ® (é€‚é…ACTæ— å›¾åƒé…ç½®)\n",
    "\n",
    "ğŸ“ ç”Ÿæˆçš„æ–‡ä»¶:\n",
    "â€¢ LeRobotæ•°æ®é›†: {converted_dataset.root}\n",
    "â€¢ ACTæ— å›¾åƒé…ç½®: /home/ps/Projects/lerobot/lerobot/common/policies/act/configuration_act_no_image.py\n",
    "â€¢ è®­ç»ƒé…ç½®ç¤ºä¾‹: /home/ps/Projects/lerobot/data/converted_dataset/training_config_example.py\n",
    "\n",
    "ğŸš€ ä¸‹ä¸€æ­¥ - è®­ç»ƒACTæ¨¡å‹:\n",
    "1. è½¬æ¢æ›´å¤šæ•°æ® (å½“å‰ä»…è½¬æ¢äº†å‰10ä¸ªepisodes)\n",
    "2. ä½¿ç”¨è®­ç»ƒè„šæœ¬è®­ç»ƒæ¨¡å‹:\n",
    "   python lerobot/scripts/train.py \\\\\n",
    "     --config-name=act_no_image \\\\\n",
    "     --dataset_repo_id={converted_dataset.repo_id}\n",
    "     \n",
    "âœ… ä»»åŠ¡å®Œæˆï¼æ•°æ®é›†å·²æˆåŠŸè½¬æ¢å¹¶å…¼å®¹ACT policyã€‚\n",
    "\"\"\")\n",
    "\n",
    "# æ˜¾ç¤ºå¦‚ä½•åŠ è½½æ•°æ®é›†è¿›è¡Œè¿›ä¸€æ­¥ä½¿ç”¨\n",
    "print(\"ğŸ”§ å¦‚ä½•åœ¨ä»£ç ä¸­ä½¿ç”¨è½¬æ¢åçš„æ•°æ®é›†:\")\n",
    "print(f\"\"\"\n",
    "from lerobot.common.datasets.lerobot_dataset import LeRobotDataset\n",
    "\n",
    "# åŠ è½½æ•°æ®é›†\n",
    "dataset = LeRobotDataset(\"{converted_dataset.repo_id}\", root=\"{converted_dataset.root.parent}\")\n",
    "\n",
    "# è·å–æ•°æ®æ ·æœ¬\n",
    "sample = dataset[0]\n",
    "print(\"è§‚æµ‹çŠ¶æ€:\", sample['observation.state'].shape)\n",
    "print(\"ç¯å¢ƒçŠ¶æ€:\", sample['observation.environment_state'].shape) \n",
    "print(\"åŠ¨ä½œ:\", sample['action'].shape)\n",
    "\n",
    "# è®­ç»ƒæ—¶å°†è‡ªåŠ¨ä½¿ç”¨è¿™äº›ç‰¹å¾\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b082f7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python lerobot/scripts/train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79611040",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ‰\" * 20)\n",
    "print(\"     è®­ç»ƒæˆåŠŸï¼ACTæ¨¡å‹æ­£åœ¨å­¦ä¹ ï¼\")\n",
    "print(\"ğŸ‰\" * 20)\n",
    "\n",
    "# æ›´æ–°è®­ç»ƒè„šæœ¬ä¸ºæ­£ç¡®çš„ç‰ˆæœ¬\n",
    "dataset_root = str(converted_dataset.root)  # ä½¿ç”¨å®Œæ•´çš„æ•°æ®é›†è·¯å¾„\n",
    "repo_id = converted_dataset.repo_id\n",
    "\n",
    "correct_train_command_final = f\"\"\"python lerobot/scripts/train.py \\\\\n",
    "    --policy.type act \\\\\n",
    "    --dataset.repo_id {repo_id} \\\\\n",
    "    --dataset.root {dataset_root} \\\\\n",
    "    --dataset.revision None \\\\\n",
    "    --policy.n_obs_steps 1 \\\\\n",
    "    --policy.chunk_size 100 \\\\\n",
    "    --policy.n_action_steps 100 \\\\\n",
    "    --batch_size 32 \\\\\n",
    "    --steps 10000 \\\\\n",
    "    --policy.optimizer_lr 1e-5 \\\\\n",
    "    --policy.use_vae true \\\\\n",
    "    --policy.kl_weight 10.0 \\\\\n",
    "    --log_freq 100 \\\\\n",
    "    --save_freq 1000 \\\\\n",
    "    --eval_freq 0\"\"\"\n",
    "\n",
    "print(\"âœ… éªŒè¯é€šè¿‡çš„è®­ç»ƒå‘½ä»¤:\")\n",
    "print(correct_train_command_final)\n",
    "\n",
    "# æ›´æ–°è®­ç»ƒè„šæœ¬æ–‡ä»¶\n",
    "train_script_path_final = (\n",
    "    \"/home/ps/Projects/lerobot/data/converted_dataset/train_act_working.sh\"\n",
    ")\n",
    "with open(train_script_path_final, \"w\") as f:\n",
    "    f.write(f\"\"\"#!/bin/bash\n",
    "# éªŒè¯æˆåŠŸçš„ACTè®­ç»ƒè„šæœ¬\n",
    "\n",
    "cd /home/ps/Projects/lerobot\n",
    "\n",
    "{correct_train_command_final}\n",
    "\"\"\")\n",
    "\n",
    "import os\n",
    "\n",
    "os.chmod(train_script_path_final, 0o755)\n",
    "\n",
    "print(f\"\\nğŸ“œ å·²åˆ›å»ºéªŒè¯æˆåŠŸçš„è®­ç»ƒè„šæœ¬: {train_script_path_final}\")\n",
    "\n",
    "print(f\"\"\"\n",
    "ğŸ† è®­ç»ƒéªŒè¯ç»“æœ:\n",
    "âœ… æ•°æ®é›†åŠ è½½æˆåŠŸ: {converted_dataset.meta.total_episodes} episodes, {converted_dataset.meta.total_frames} frames\n",
    "âœ… ACTæ¨¡å‹åˆ›å»ºæˆåŠŸ: 40M å‚æ•°\n",
    "âœ… è®­ç»ƒå¾ªç¯æ­£å¸¸è¿è¡Œ\n",
    "âœ… Lossä»29.551é™åˆ°4.049 (100æ­¥å†…)\n",
    "âœ… Gradient normç¨³å®šåœ¨119-565èŒƒå›´\n",
    "âœ… æ£€æŸ¥ç‚¹ä¿å­˜æˆåŠŸ\n",
    "\n",
    "ğŸ¯ å…³é”®å‘ç°:\n",
    "â€¢ æ•°æ®é›†è·¯å¾„éœ€è¦ä½¿ç”¨å®Œæ•´è·¯å¾„: {dataset_root}\n",
    "â€¢ å¸ƒå°”å‚æ•°ä½¿ç”¨å°å†™: --policy.use_vae true\n",
    "â€¢ revisionå‚æ•°éœ€è¦æ˜ç¡®è®¾ç½®: --dataset.revision None\n",
    "\n",
    "ğŸš€ ä¸‹ä¸€æ­¥å»ºè®®:\n",
    "1. å¢åŠ è®­ç»ƒæ­¥æ•° (--steps 50000+)\n",
    "2. è°ƒæ•´batch_sizeæ ¹æ®GPUå†…å­˜\n",
    "3. ç›‘æ§lossæ›²çº¿å’Œgradient norm\n",
    "4. è€ƒè™‘æ·»åŠ evaluationç¯å¢ƒ\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nğŸ ä»»åŠ¡å®Œæˆ! HDF5 â†’ LeRobot â†’ ACT è®­ç»ƒæµç¨‹å…¨éƒ¨æ‰“é€š!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fca25ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"ğŸ” ACTè®­ç»ƒæ—¶çš„æ•°æ®é¢„å¤„ç†æµç¨‹åˆ†æ\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\"\"\n",
    "ğŸ“‹ é—®é¢˜ï¼šACTä¼šè‡ªåŠ¨è¿›è¡Œæ•°æ®é¢„å¤„ç†å—ï¼Ÿè¿˜æ˜¯éœ€è¦æ‰‹åŠ¨å¤„ç†ï¼Ÿ\n",
    "\n",
    "ğŸ¯ ç­”æ¡ˆï¼šLeRobot/ACTæœ‰å®Œæ•´çš„è‡ªåŠ¨æ•°æ®é¢„å¤„ç†æµç¨‹ï¼\n",
    "\n",
    "è®©æˆ‘ä»¬æ·±å…¥åˆ†æè¿™ä¸ªæµç¨‹ï¼š\n",
    "\"\"\")\n",
    "\n",
    "# 1. åˆ†æLeRobotæ•°æ®é›†çš„é¢„å¤„ç†æœºåˆ¶\n",
    "print(\"1ï¸âƒ£ LeRobotæ•°æ®é›†é¢„å¤„ç†æœºåˆ¶\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "from lerobot.common.datasets.lerobot_dataset import LeRobotDataset\n",
    "from lerobot.common.policies.factory import make_policy\n",
    "\n",
    "# åŠ è½½æˆ‘ä»¬çš„æ•°æ®é›†çœ‹çœ‹é¢„å¤„ç†æµç¨‹\n",
    "dataset = converted_dataset\n",
    "print(f\"æ•°æ®é›†ä¿¡æ¯:\")\n",
    "print(f\"  - Features: {list(dataset.features.keys())}\")\n",
    "print(f\"  - Statså­˜åœ¨: {hasattr(dataset, 'stats')}\")\n",
    "print(f\"  - FPS: {dataset.fps}\")\n",
    "\n",
    "# æ£€æŸ¥statsæ–‡ä»¶\n",
    "import json\n",
    "\n",
    "stats_file = dataset.root / \"meta\" / \"stats.json\"\n",
    "if stats_file.exists():\n",
    "    with open(stats_file, \"r\") as f:\n",
    "        stats = json.load(f)\n",
    "    print(f\"  - Stats keys: {list(stats.keys())}\")\n",
    "else:\n",
    "    print(\"  - No stats.json found (éœ€è¦è®¡ç®—)\")\n",
    "\n",
    "print(f\"\\nğŸ“Š åŸå§‹æ•°æ®èŒƒå›´ (ç¬¬ä¸€ä¸ªæ ·æœ¬):\")\n",
    "sample = dataset[0]\n",
    "for key, value in sample.items():\n",
    "    if hasattr(value, \"shape\"):\n",
    "        print(f\"  {key}:\")\n",
    "        print(f\"    Shape: {value.shape}\")\n",
    "        print(f\"    Dtype: {value.dtype}\")\n",
    "\n",
    "        if value.dtype == bool:\n",
    "            print(f\"    Values: {value}\")\n",
    "        elif hasattr(value, \"min\") and value.dtype in [\n",
    "            \"float32\",\n",
    "            \"float64\",\n",
    "            \"int32\",\n",
    "            \"int64\",\n",
    "        ]:\n",
    "            print(f\"    Range: [{value.min():.3f}, {value.max():.3f}]\")\n",
    "            if value.numel() > 1:  # ç¡®ä¿ä¸æ˜¯æ ‡é‡ä¸”æœ‰å¤šä¸ªå…ƒç´ \n",
    "                print(\n",
    "                    f\"    Mean: {value.float().mean():.3f}, Std: {value.float().std():.3f}\"\n",
    "                )\n",
    "            else:\n",
    "                print(f\"    Value: {value.item():.3f}\")\n",
    "        else:\n",
    "            print(f\"    Values: {value}\")\n",
    "    else:\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "print(f\"\\n2ï¸âƒ£ ACT Policyçš„normalizationé…ç½®\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# æ£€æŸ¥ACTé…ç½®ä¸­çš„normalizationè®¾ç½®\n",
    "from lerobot.common.policies.act.configuration_act import ACTConfig\n",
    "\n",
    "# åˆ›å»ºACT configæŸ¥çœ‹é»˜è®¤è®¾ç½®\n",
    "act_config = ACTConfig()\n",
    "print(f\"Normalization mapping: {act_config.normalization_mapping}\")\n",
    "\n",
    "print(f\"\"\"\n",
    "ğŸ”§ LeRobotçš„è‡ªåŠ¨é¢„å¤„ç†åŒ…æ‹¬ï¼š\n",
    "\n",
    "1. æ•°æ®æ ¼å¼è½¬æ¢:\n",
    "   âœ… è‡ªåŠ¨å°†numpyæ•°ç»„è½¬ä¸ºtorch tensor\n",
    "   âœ… è‡ªåŠ¨ç§»åŠ¨æ•°æ®åˆ°GPU (å¦‚æœæŒ‡å®š)\n",
    "   âœ… è‡ªåŠ¨æ‰¹å¤„ç† (batch_sizeç»´åº¦)\n",
    "\n",
    "2. æ•°æ®æ ‡å‡†åŒ– (Normalization):\n",
    "   âœ… æ ¹æ®normalization_mappingè‡ªåŠ¨æ ‡å‡†åŒ–\n",
    "   âœ… STATEç‰¹å¾ â†’ MEAN_STDæ ‡å‡†åŒ– (å‡å‡å€¼é™¤æ ‡å‡†å·®)\n",
    "   âœ… ACTIONç‰¹å¾ â†’ MEAN_STDæ ‡å‡†åŒ–\n",
    "   âœ… VISUALç‰¹å¾ â†’ MEAN_STDæ ‡å‡†åŒ– (å¦‚æœæœ‰å›¾åƒ)\n",
    "\n",
    "3. æ—¶åºå¤„ç†:\n",
    "   âœ… æ ¹æ®n_obs_stepsè‡ªåŠ¨å¤„ç†è§‚æµ‹å†å²\n",
    "   âœ… æ ¹æ®chunk_sizeè‡ªåŠ¨å¤„ç†åŠ¨ä½œåºåˆ—\n",
    "\n",
    "4. ç‰¹æ®Šå¤„ç†:\n",
    "   âœ… è‡ªåŠ¨padding (å¦‚æœéœ€è¦)\n",
    "   âœ… è‡ªåŠ¨maskå¤„ç† (å¯¹äºå˜é•¿åºåˆ—)\n",
    "\"\"\")\n",
    "\n",
    "print(f\"\\n3ï¸âƒ£ ä½ éœ€è¦åšçš„é¢„å¤„ç† vs è‡ªåŠ¨å¤„ç†\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "print(f\"\"\"\n",
    "âŒ ä½ ä¸éœ€è¦æ‰‹åŠ¨åšçš„:\n",
    "  - æ•°æ®æ ‡å‡†åŒ– (z-score normalization)\n",
    "  - æ•°æ®ç±»å‹è½¬æ¢ (numpy â†’ torch)\n",
    "  - æ‰¹å¤„ç†ç»´åº¦æ·»åŠ \n",
    "  - GPUæ•°æ®ä¼ è¾“\n",
    "  - æ—¶åºæ•°æ®ç»„ç»‡\n",
    "  - Paddingå’Œmasking\n",
    "\n",
    "âœ… ä½ å·²ç»åšè¿‡çš„ (åœ¨è½¬æ¢HDF5æ—¶):\n",
    "  - å°†åŸå§‹ä¼ æ„Ÿå™¨æ•°æ®ç»„ç»‡ä¸ºåˆé€‚çš„ç‰¹å¾\n",
    "  - å®šä¹‰observation.stateå’Œobservation.environment_state\n",
    "  - ç¡®ä¿åŠ¨ä½œæ•°æ®æ ¼å¼æ­£ç¡®\n",
    "  - åˆ›å»ºepisodeå’Œframeç»“æ„\n",
    "\n",
    "âš™ï¸  ä½ å¯èƒ½éœ€è¦è°ƒæ•´çš„:\n",
    "  - normalization_mapping (å¦‚æœé»˜è®¤ä¸åˆé€‚)\n",
    "  - ç‰¹å¾ç»„åˆæ–¹å¼ (å¦‚æœæƒ³æ”¹å˜stateç‰¹å¾)\n",
    "  - æ•°æ®é‡‡æ ·ç­–ç•¥ (å¦‚æœæƒ³æ”¹å˜è®­ç»ƒé‡‡æ ·æ–¹å¼)\n",
    "\"\"\")\n",
    "\n",
    "# è®©æˆ‘ä»¬çœ‹çœ‹æ•°æ®åœ¨è®­ç»ƒæ—¶æ˜¯å¦‚ä½•è¢«å¤„ç†çš„\n",
    "print(f\"\\n4ï¸âƒ£ å®é™…è®­ç»ƒæ—¶çš„æ•°æ®æµ\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# æ¨¡æ‹Ÿä¸€ä¸ªbatchçš„å¤„ç†\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# åˆ›å»ºä¸€ä¸ªç®€å•çš„dataloaderçœ‹æ•°æ®æ ¼å¼\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=False)\n",
    "batch = next(iter(dataloader))\n",
    "\n",
    "print(f\"Batchç»“æ„:\")\n",
    "for key, value in batch.items():\n",
    "    if hasattr(value, \"shape\"):\n",
    "        print(f\"  {key}: {value.shape} {value.dtype}\")\n",
    "    else:\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "print(f\"\"\"\n",
    "ğŸ“ˆ æ•°æ®æµæ¦‚è¿°:\n",
    "Raw HDF5 â†’ LeRobot Dataset â†’ DataLoader â†’ Batch â†’ ACT Policy â†’ Loss\n",
    "\n",
    "åœ¨è¿™ä¸ªæµç¨‹ä¸­:\n",
    "1. LeRobot Datasetè´Ÿè´£è¯»å–å’ŒåŸºç¡€è½¬æ¢\n",
    "2. DataLoaderè´Ÿè´£æ‰¹å¤„ç†å’Œshuffle\n",
    "3. ACT Policyå†…éƒ¨è¿›è¡Œnormalizationå’Œforwardè®¡ç®—\n",
    "4. æ‰€æœ‰å¤æ‚çš„é¢„å¤„ç†éƒ½æ˜¯è‡ªåŠ¨çš„ï¼\n",
    "\n",
    "ğŸ¯ ç»“è®ºï¼šä½ å‡ ä¹ä¸éœ€è¦æ‰‹åŠ¨é¢„å¤„ç†ï¼\n",
    "   LeRobotå·²ç»ä¸ºä½ å¤„ç†äº†99%çš„æ•°æ®é¢„å¤„ç†å·¥ä½œã€‚\n",
    "\"\"\")\n",
    "\n",
    "print(f\"\\n5ï¸âƒ£ å¦‚ä½•æŸ¥çœ‹å’Œè°ƒæ•´é¢„å¤„ç†\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "print(f\"\"\"\n",
    "å¦‚æœä½ æƒ³æŸ¥çœ‹æˆ–è°ƒæ•´é¢„å¤„ç†ï¼š\n",
    "\n",
    "1. æŸ¥çœ‹normalization stats:\n",
    "   dataset.stats æˆ– dataset.root/meta/stats.json\n",
    "\n",
    "2. ä¿®æ”¹normalizationæ–¹å¼:\n",
    "   åœ¨è®­ç»ƒå‘½ä»¤ä¸­æ·»åŠ :\n",
    "   --policy.normalization_mapping.STATE=MIN_MAX\n",
    "   --policy.normalization_mapping.ACTION=MIN_MAX\n",
    "\n",
    "3. è‡ªå®šä¹‰normalization mapping:\n",
    "   ç¼–è¾‘ACTé…ç½®æ–‡ä»¶ä¸­çš„normalization_mapping\n",
    "\n",
    "4. æŸ¥çœ‹å®é™…çš„normalizedæ•°æ®:\n",
    "   åœ¨policy forwardä¸­æ·»åŠ æ‰“å°è¯­å¥\n",
    "\n",
    "5. æ•°æ®å¢å¼º (å¦‚æœæœ‰å›¾åƒ):\n",
    "   --dataset.image_transforms.enable=true\n",
    "\"\"\")\n",
    "\n",
    "print(f\"\\nâœ… æ€»ç»“ï¼šLeRobotçš„ACTå®ç°å·²ç»ä¸ºä½ è‡ªåŠ¨å¤„ç†äº†æ‰€æœ‰ä¸»è¦çš„æ•°æ®é¢„å¤„ç†ï¼\")\n",
    "print(f\"   ä½ åªéœ€è¦å…³æ³¨æ¨¡å‹è®­ç»ƒå’Œè¶…å‚æ•°è°ƒä¼˜ã€‚\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9214c23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"ğŸ”¬ æ·±å…¥åˆ†æACTå†…éƒ¨æ•°æ®é¢„å¤„ç†æºç \")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# è®©æˆ‘ä»¬æŸ¥çœ‹ACT policyçš„å®é™…é¢„å¤„ç†ä»£ç \n",
    "from lerobot.common.policies.act.modeling_act import ACTPolicy\n",
    "import inspect\n",
    "\n",
    "print(\"1ï¸âƒ£ ACT Policyç±»çš„å…³é”®æ–¹æ³•\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# åˆ—å‡ºACT Policyçš„ä¸»è¦æ–¹æ³•\n",
    "act_methods = [method for method in dir(ACTPolicy) if not method.startswith(\"_\")]\n",
    "print(f\"ACT Policyä¸»è¦æ–¹æ³•: {act_methods}\")\n",
    "\n",
    "print(f\"\\n2ï¸âƒ£ forwardæ–¹æ³•åˆ†æ (æ•°æ®å¤„ç†çš„æ ¸å¿ƒ)\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# è·å–forwardæ–¹æ³•çš„æºç \n",
    "try:\n",
    "    forward_source = inspect.getsource(ACTPolicy.forward)\n",
    "    print(\"Forwardæ–¹æ³•æºç ç‰‡æ®µ (å‰50è¡Œ):\")\n",
    "    lines = forward_source.split(\"\\n\")\n",
    "    for i, line in enumerate(lines[:50]):\n",
    "        print(f\"{i + 1:2d}: {line}\")\n",
    "    if len(lines) > 50:\n",
    "        print(f\"... (è¿˜æœ‰{len(lines) - 50}è¡Œ)\")\n",
    "except:\n",
    "    print(\"æ— æ³•è·å–æºç ï¼Œä½†æˆ‘ä»¬å¯ä»¥é€šè¿‡å…¶ä»–æ–¹å¼åˆ†æ\")\n",
    "\n",
    "print(f\"\\n3ï¸âƒ£ æ•°æ®æ ‡å‡†åŒ– (Normalization) æµç¨‹\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# æŸ¥çœ‹normalizationç›¸å…³çš„ä»£ç \n",
    "from lerobot.common.policies.normalize import Normalize, Unnormalize\n",
    "\n",
    "print(\"Normalizeç±»è´Ÿè´£æ•°æ®æ ‡å‡†åŒ–:\")\n",
    "try:\n",
    "    # æ£€æŸ¥normalizeçš„forwardæ–¹æ³•\n",
    "    normalize_source = inspect.getsource(Normalize.forward)\n",
    "    print(\"Normalize.forwardæ–¹æ³•å‰15è¡Œ:\")\n",
    "    lines = normalize_source.split(\"\\n\")\n",
    "    for i, line in enumerate(lines[:15]):\n",
    "        print(f\"{i + 1:2d}: {line}\")\n",
    "    if len(lines) > 15:\n",
    "        print(f\"... (è¿˜æœ‰{len(lines) - 15}è¡Œ)\")\n",
    "except Exception as e:\n",
    "    print(f\"æ— æ³•è·å–æºç : {e}\")\n",
    "    print(\"Normalizeç±»å¤„ç†æ•°æ®æ ‡å‡†åŒ–ï¼ŒåŒ…æ‹¬MEAN_STDå’ŒMIN_MAXä¸¤ç§æ¨¡å¼\")\n",
    "\n",
    "print(f\"\\n4ï¸âƒ£ å®é™…æ•°æ®æµæµ‹è¯•\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# åˆ›å»ºä¸€ä¸ªå°çš„ACT policyæ¥æµ‹è¯•æ•°æ®æµ\n",
    "try:\n",
    "    from lerobot.common.policies.act.configuration_act import ACTConfig\n",
    "\n",
    "    # ä½¿ç”¨æˆ‘ä»¬æ•°æ®é›†çš„metaä¿¡æ¯\n",
    "    test_config = ACTConfig()\n",
    "\n",
    "    # æ¨¡æ‹Ÿåˆ›å»ºpolicy (ä¸å®é™…åŠ è½½ï¼Œåªåˆ†æé…ç½®)\n",
    "    print(f\"ACTé…ç½®ç”¨äºæ•°æ®é¢„å¤„ç†çš„å…³é”®å‚æ•°:\")\n",
    "    print(f\"  - n_obs_steps: {test_config.n_obs_steps} (è§‚æµ‹å†å²é•¿åº¦)\")\n",
    "    print(f\"  - chunk_size: {test_config.chunk_size} (åŠ¨ä½œåºåˆ—é•¿åº¦)\")\n",
    "    print(f\"  - normalization_mapping: {test_config.normalization_mapping}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"æ— æ³•åˆ›å»ºæµ‹è¯•policy: {e}\")\n",
    "\n",
    "print(f\"\\n5ï¸âƒ£ LeRobotæ•°æ®æ ‡å‡†åŒ–ç»Ÿè®¡è®¡ç®—\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# æŸ¥çœ‹æ˜¯å¦å·²ç»è®¡ç®—äº†æ•°æ®ç»Ÿè®¡\n",
    "stats_file = converted_dataset.root / \"meta\" / \"stats.json\"\n",
    "if stats_file.exists():\n",
    "    import json\n",
    "\n",
    "    with open(stats_file, \"r\") as f:\n",
    "        stats = json.load(f)\n",
    "\n",
    "    print(\"å·²å­˜åœ¨çš„æ•°æ®ç»Ÿè®¡:\")\n",
    "    for feature, stat_info in stats.items():\n",
    "        if isinstance(stat_info, dict):\n",
    "            print(f\"  {feature}:\")\n",
    "            for stat_name, values in stat_info.items():\n",
    "                if isinstance(values, list) and len(values) <= 10:\n",
    "                    print(f\"    {stat_name}: {values}\")\n",
    "                else:\n",
    "                    print(\n",
    "                        f\"    {stat_name}: {type(values)} (length={len(values) if hasattr(values, '__len__') else 'N/A'})\"\n",
    "                    )\n",
    "else:\n",
    "    print(\"âš ï¸  æ•°æ®ç»Ÿè®¡æ–‡ä»¶ä¸å­˜åœ¨ï¼Œä¼šåœ¨é¦–æ¬¡è®­ç»ƒæ—¶è‡ªåŠ¨è®¡ç®—\")\n",
    "    print(\"   è¿™åŒ…æ‹¬æ¯ä¸ªç‰¹å¾çš„ mean, std, min, max ç­‰ç»Ÿè®¡ä¿¡æ¯\")\n",
    "\n",
    "print(f\"\\n6ï¸âƒ£ å…³é”®å‘ç°æ€»ç»“\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "print(f\"\"\"\n",
    "ğŸ” ACTçš„è‡ªåŠ¨æ•°æ®é¢„å¤„ç†åŒ…æ‹¬ï¼š\n",
    "\n",
    "A. è¾“å…¥é¢„å¤„ç† (åœ¨Policy.forwardä¸­):\n",
    "   1. ç‰¹å¾æå–å’Œç»„åˆ\n",
    "   2. è‡ªåŠ¨æ ‡å‡†åŒ– (æ ¹æ®normalization_mapping)\n",
    "   3. æ—¶åºæ•°æ®ç»„ç»‡ (n_obs_steps)\n",
    "   4. å¼ é‡ç»´åº¦è°ƒæ•´\n",
    "\n",
    "B. æ ‡å‡†åŒ–æ–¹å¼:\n",
    "   - MEAN_STD: (x - mean) / std (é»˜è®¤)\n",
    "   - MIN_MAX: (x - min) / (max - min) * 2 - 1\n",
    "\n",
    "C. è¾“å‡ºé¢„å¤„ç†:\n",
    "   - åŠ¨ä½œåºåˆ—ç”Ÿæˆ (chunk_size)\n",
    "   - VAEç¼–ç /è§£ç  (å¦‚æœå¯ç”¨)\n",
    "   - é€†æ ‡å‡†åŒ–è¾“å‡ºåŠ¨ä½œ\n",
    "\n",
    "D. ä½ æ— éœ€æ‹…å¿ƒçš„:\n",
    "   âœ… æ•°æ®ç±»å‹è½¬æ¢\n",
    "   âœ… æ‰¹é‡ç»´åº¦å¤„ç†\n",
    "   âœ… GPUå†…å­˜ä¼ è¾“\n",
    "   âœ… æ ‡å‡†åŒ–ç»Ÿè®¡è®¡ç®—\n",
    "   âœ… ç¼ºå¤±å€¼å¤„ç†\n",
    "   âœ… æ—¶åºå¯¹é½\n",
    "\n",
    "E. ä½ å¯ä»¥æ§åˆ¶çš„:\n",
    "   âš™ï¸  normalization_mapping (é€šè¿‡é…ç½®)\n",
    "   âš™ï¸  chunk_sizeå’Œn_obs_steps (é€šè¿‡é…ç½®)\n",
    "   âš™ï¸  ç‰¹å¾ç»„åˆæ–¹å¼ (é€šè¿‡æ•°æ®é›†ç‰¹å¾å®šä¹‰)\n",
    "\n",
    "ğŸ¯ ç»“è®º: LeRobot/ACTçš„æ•°æ®é¢„å¤„ç†æ˜¯å…¨è‡ªåŠ¨çš„ï¼\n",
    "   ä½ åªéœ€è¦ç¡®ä¿åŸå§‹æ•°æ®æ ¼å¼æ­£ç¡®ï¼ˆå·²å®Œæˆâœ…ï¼‰ï¼Œ\n",
    "   å…¶ä½™çš„é¢„å¤„ç†éƒ½ç”±ç³»ç»Ÿè‡ªåŠ¨å¤„ç†ã€‚\n",
    "\"\"\")\n",
    "\n",
    "print(f\"\\nğŸ’¡ å¦‚æœä½ æƒ³è‡ªå®šä¹‰é¢„å¤„ç†:\")\n",
    "print(f\"   1. ä¿®æ”¹normalization_mapping\")\n",
    "print(f\"   2. åœ¨è½¬æ¢æ•°æ®æ—¶è°ƒæ•´ç‰¹å¾ç»„åˆ\")\n",
    "print(f\"   3. åˆ›å»ºè‡ªå®šä¹‰çš„Datasetå­ç±»\")\n",
    "print(f\"   4. ä¿®æ”¹ACT policyçš„forwardæ–¹æ³•\")\n",
    "print(f\"\\n   ä½†å¯¹äºå¤§å¤šæ•°åœºæ™¯ï¼Œé»˜è®¤çš„è‡ªåŠ¨é¢„å¤„ç†å°±è¶³å¤Ÿäº†ï¼\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ccdd811",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"ğŸ› ï¸ å®é™…æ§åˆ¶ACTæ•°æ®é¢„å¤„ç†çš„æ–¹æ³•\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\"\"\n",
    "è™½ç„¶ACTä¼šè‡ªåŠ¨å¤„ç†æ•°æ®é¢„å¤„ç†ï¼Œä½†ä½ ä¹Ÿå¯ä»¥æ ¹æ®éœ€è¦è¿›è¡Œæ§åˆ¶å’Œè‡ªå®šä¹‰ï¼š\n",
    "\"\"\")\n",
    "\n",
    "print(\"1ï¸âƒ£ æŸ¥çœ‹å½“å‰çš„æ•°æ®ç»Ÿè®¡ä¿¡æ¯\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# å®é™…è®¡ç®—ä¸€äº›æ•°æ®ç»Ÿè®¡æ¥æ¼”ç¤º\n",
    "sample_batch = []\n",
    "for i in range(min(100, len(converted_dataset))):  # å–å‰100ä¸ªæ ·æœ¬\n",
    "    sample_batch.append(converted_dataset[i])\n",
    "\n",
    "# è®¡ç®—è§‚æµ‹çŠ¶æ€çš„ç»Ÿè®¡ä¿¡æ¯\n",
    "obs_state_values = [s[\"observation.state\"].numpy() for s in sample_batch]\n",
    "env_state_values = [s[\"observation.environment_state\"].numpy() for s in sample_batch]\n",
    "action_values = [s[\"action\"].numpy() for s in sample_batch]\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "obs_state_array = np.stack(obs_state_values)\n",
    "env_state_array = np.stack(env_state_values)\n",
    "action_array = np.stack(action_values)\n",
    "\n",
    "print(f\"åŸºäºå‰{len(sample_batch)}ä¸ªæ ·æœ¬çš„ç»Ÿè®¡:\")\n",
    "print(f\"  observation.state:\")\n",
    "print(f\"    Shape: {obs_state_array.shape}\")\n",
    "print(f\"    Mean: {obs_state_array.mean(axis=0)[:5]} ... (å‰5ç»´)\")\n",
    "print(f\"    Std:  {obs_state_array.std(axis=0)[:5]} ... (å‰5ç»´)\")\n",
    "print(f\"    Range: [{obs_state_array.min():.3f}, {obs_state_array.max():.3f}]\")\n",
    "\n",
    "print(f\"  observation.environment_state:\")\n",
    "print(f\"    Shape: {env_state_array.shape}\")\n",
    "print(f\"    Mean: {env_state_array.mean(axis=0)}\")\n",
    "print(f\"    Std:  {env_state_array.std(axis=0)}\")\n",
    "print(f\"    Range: [{env_state_array.min():.3f}, {env_state_array.max():.3f}]\")\n",
    "\n",
    "print(f\"  action:\")\n",
    "print(f\"    Shape: {action_array.shape}\")\n",
    "print(f\"    Mean: {action_array.mean(axis=0)}\")\n",
    "print(f\"    Std:  {action_array.std(axis=0)}\")\n",
    "print(f\"    Range: [{action_array.min():.3f}, {action_array.max():.3f}]\")\n",
    "\n",
    "print(f\"\\n2ï¸âƒ£ è‡ªå®šä¹‰normalizationçš„æ–¹æ³•\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "print(f\"\"\"\n",
    "å¦‚æœä½ æƒ³è‡ªå®šä¹‰æ•°æ®é¢„å¤„ç†ï¼Œå¯ä»¥ï¼š\n",
    "\n",
    "A. ä¿®æ”¹è®­ç»ƒå‘½ä»¤ä¸­çš„normalization:\n",
    "   python lerobot/scripts/train.py \\\\\n",
    "     --policy.normalization_mapping.STATE=MIN_MAX \\\\\n",
    "     --policy.normalization_mapping.ACTION=MIN_MAX \\\\\n",
    "     ... (å…¶ä»–å‚æ•°)\n",
    "\n",
    "B. åˆ›å»ºè‡ªå®šä¹‰çš„ACTé…ç½®:\n",
    "\"\"\")\n",
    "\n",
    "# å±•ç¤ºå¦‚ä½•åˆ›å»ºè‡ªå®šä¹‰é…ç½®\n",
    "custom_config_example = \"\"\"\n",
    "from lerobot.common.policies.act.configuration_act import ACTConfig\n",
    "from lerobot.configs.types import NormalizationMode\n",
    "\n",
    "class CustomACTConfig(ACTConfig):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # è‡ªå®šä¹‰normalization\n",
    "        self.normalization_mapping = {\n",
    "            \"STATE\": NormalizationMode.MIN_MAX,      # ä½¿ç”¨MIN_MAXè€Œä¸æ˜¯MEAN_STD\n",
    "            \"ACTION\": NormalizationMode.MEAN_STD,    # åŠ¨ä½œä»ç”¨MEAN_STD\n",
    "        }\n",
    "        # å…¶ä»–è‡ªå®šä¹‰å‚æ•°\n",
    "        self.chunk_size = 50  # æ”¹å˜åŠ¨ä½œåºåˆ—é•¿åº¦\n",
    "\"\"\"\n",
    "\n",
    "print(\"è‡ªå®šä¹‰é…ç½®ç¤ºä¾‹:\")\n",
    "print(custom_config_example)\n",
    "\n",
    "print(f\"\\n3ï¸âƒ£ æ•°æ®é¢„å¤„ç†çš„éªŒè¯æ–¹æ³•\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "print(f\"\"\"\n",
    "éªŒè¯æ•°æ®é¢„å¤„ç†æ˜¯å¦æ­£ç¡®ï¼š\n",
    "\n",
    "1. æŸ¥çœ‹è®­ç»ƒæ—¥å¿—ä¸­çš„losså˜åŒ–\n",
    "   - Lossåº”è¯¥é€æ¸ä¸‹é™\n",
    "   - å¦‚æœlossçˆ†ç‚¸æˆ–ä¸æ”¶æ•›ï¼Œå¯èƒ½æ˜¯normalizationé—®é¢˜\n",
    "\n",
    "2. æ£€æŸ¥é¢„æµ‹åŠ¨ä½œçš„èŒƒå›´\n",
    "   - ç¡®ä¿é¢„æµ‹çš„åŠ¨ä½œåœ¨åˆç†èŒƒå›´å†…\n",
    "   - å¯ä»¥åœ¨è®­ç»ƒä¸­æ‰“å°actionçš„ç»Ÿè®¡ä¿¡æ¯\n",
    "\n",
    "3. å¯è§†åŒ–æ•°æ®åˆ†å¸ƒ\n",
    "   - è®­ç»ƒå‰åæ•°æ®çš„å‡å€¼å’Œæ–¹å·®\n",
    "   - ç¡®ä¿æ ‡å‡†åŒ–åæ•°æ®åˆ†å¸ƒåˆç† (é€šå¸¸æ¥è¿‘æ ‡å‡†æ­£æ€åˆ†å¸ƒ)\n",
    "\n",
    "4. ä½¿ç”¨tensorboardæˆ–wandbç›‘æ§\n",
    "   - è§‚å¯Ÿå„ä¸ªç‰¹å¾çš„ç»Ÿè®¡ä¿¡æ¯\n",
    "   - ç›‘æ§gradient normå’Œloss\n",
    "\"\"\")\n",
    "\n",
    "print(f\"\\n4ï¸âƒ£ å¸¸è§çš„æ•°æ®é¢„å¤„ç†é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆ\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "print(f\"\"\"\n",
    "âŒ å¸¸è§é—®é¢˜:\n",
    "1. åŠ¨ä½œå€¼èŒƒå›´è¿‡å¤§/è¿‡å°\n",
    "   â†’ æ£€æŸ¥åŸå§‹åŠ¨ä½œæ•°æ®çš„å•ä½å’ŒèŒƒå›´\n",
    "   â†’ è€ƒè™‘ä½¿ç”¨MIN_MAX normalization\n",
    "\n",
    "2. ä¸åŒç‰¹å¾çš„å°ºåº¦å·®å¼‚å¾ˆå¤§\n",
    "   â†’ ç¡®ä¿æ‰€æœ‰ç‰¹å¾éƒ½è¿›è¡Œäº†é€‚å½“çš„æ ‡å‡†åŒ–\n",
    "   â†’ æ£€æŸ¥normalization_mappingé…ç½®\n",
    "\n",
    "3. Lossä¸æ”¶æ•›æˆ–çˆ†ç‚¸\n",
    "   â†’ é™ä½å­¦ä¹ ç‡\n",
    "   â†’ æ£€æŸ¥gradient clippingè®¾ç½®\n",
    "   â†’ éªŒè¯æ•°æ®ä¸­æ˜¯å¦æœ‰å¼‚å¸¸å€¼\n",
    "\n",
    "4. é¢„æµ‹åŠ¨ä½œä¸åˆç†\n",
    "   â†’ æ£€æŸ¥actionçš„é€†æ ‡å‡†åŒ–æ˜¯å¦æ­£ç¡®\n",
    "   â†’ éªŒè¯è®­ç»ƒæ•°æ®çš„è´¨é‡\n",
    "\n",
    "âœ… è§£å†³æ–¹æ¡ˆ:\n",
    "- å¤§å¤šæ•°é—®é¢˜éƒ½å¯ä»¥é€šè¿‡è°ƒæ•´normalizationæ–¹å¼è§£å†³\n",
    "- LeRobotçš„é»˜è®¤è®¾ç½®å¯¹å¤§å¤šæ•°æœºå™¨äººä»»åŠ¡éƒ½æœ‰æ•ˆ\n",
    "- å…³é”®æ˜¯ç¡®ä¿åŸå§‹æ•°æ®çš„è´¨é‡å’Œæ ¼å¼æ­£ç¡® (ä½ å·²ç»åšåˆ°äº†âœ…)\n",
    "\"\"\")\n",
    "\n",
    "print(f\"\\nğŸ¯ æœ€ç»ˆå»ºè®®:\")\n",
    "print(f\"   1. ä½¿ç”¨é»˜è®¤çš„è‡ªåŠ¨é¢„å¤„ç†å¼€å§‹è®­ç»ƒ\")\n",
    "print(f\"   2. ç›‘æ§è®­ç»ƒæŒ‡æ ‡ (loss, gradient norm)\")\n",
    "print(f\"   3. åªæœ‰åœ¨é‡åˆ°æ˜æ˜¾é—®é¢˜æ—¶æ‰è€ƒè™‘è‡ªå®šä¹‰é¢„å¤„ç†\")\n",
    "print(f\"   4. ä½ çš„æ•°æ®è½¬æ¢å·²ç»å¾ˆå¥½ï¼Œé¢„å¤„ç†åº”è¯¥ä¸ä¼šæœ‰é—®é¢˜ï¼\")\n",
    "\n",
    "print(f\"\\nâœ… æ€»ç»“: ACTçš„æ•°æ®é¢„å¤„ç†æ˜¯å¯é å’Œè‡ªåŠ¨çš„ï¼Œä½ å¯ä»¥ä¸“æ³¨äºæ¨¡å‹è®­ç»ƒï¼\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "isaaclab_isaacsim_4.5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
